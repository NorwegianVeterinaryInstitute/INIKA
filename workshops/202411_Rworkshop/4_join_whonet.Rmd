---
title: "`r params$title`"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author: Eve Zeyl Fiskebeck and Madelaine Norström
params:
  title: "Chap 4. Data from a WHONET database and merging datasets." 
  project_path: "`r here::here()`"
  

knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../../docs") })  
  
output: 
  
  rmdformats::readthedown:
      
      css: ../style.css
      self_contained: true
      code_download: true
      toc_depth: 4
      df_print: paged
      code_folding: show
      author: params$author
      highlight: espresso
      number_sections: true
      
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, 
                      message=FALSE, warning=FALSE,
                      results = 'hide')
```



```{r libraries import, class.source = "fold-hide"}
library(here)
library(tidyverse)
library(RSQLite)
```


# Working with data from SQLite database in R

## Finding where WHONET database is stored

The data you will have registered in WHONET software are stored in a SQLite database.
It is possible to have different databases in WHONET, depending on which 
laboratory you are working with.

<!-- We need to have WHONET lesson before this course --> 

If you installed WHONET program using the default settings, you should will find 
the test database in the following path: `C:/WHONET/Data`. Please check that
you find the test database that was used for the registering exercise. 
If you saved the database in another directory, please note the path 
(eg. use the file explorer program to find the file, and copy the path as text)


The path I copied is `C:\WHONET` and the my test database is called `WHO-TST-2020-OneHealth.sqlite` (the extension is explicit here).

I first need to check if I manage to explore the directory that should contain
the database and list the files that are contained in this directory using R.
(This serves as test for accessing paths and files in windows system).


### Listing files in a directory

Note: I using variable name that are explicit and descriptive of the content. 
This helps making the code more readable. I am not afraid of using long names, 
as I can use tab completion in RStudio to avoid typing long names. 

```{r}
whonet_data_path <- "C:/WHONET/Data"
list.files(whonet_data_path)
```


## Using data directly from a SQLite database

We will use the packages `RSQLite` and `dbplyr` (a part of the tidyverse) to work with
SQLite databases. `RSQLite` is a package that allows R to connect to SQLite databases
and `dbplyr` is a package that allows to send instructions (queries) to the 
SQLite database using dplyr language that you learned during last lesson. 
In other words, `dbplyr` translates the query you write into a SQL query language. 


To be able to read the data stored in the SQL database, we need to create
a connection to the SQL database. You can see this as a bridge between the database
and R memory, allowing information to circulate from (and to) the database to R.

> Note: We won't show you how to write back data in the SQLite database. 
We do not want that because we do not want you to modify the raw data. But it is
totally possible. It is also possible to create another database or another table to contain the cleaned data. 


### Creating a connection to the sqlite database

```{r}
# dbConnect - is an imported function from DBI
dbconn <- dbConnect(RSQLite::SQLite(), 
                    here::here(whonet_data_path, "TZA-INIKA_TZ-2024.sqlite"))

str(dbconn)
print(dbconn)
```
> dbconn is not the data contained in the database, but a connection, 
that will allow to retrive the information contained in the database file.

Note: look how the path of the file is represented here. If you one day get 
problem with reading files in R from windows, this is how you have to treat the
paths, using `\\` instead of `/`. Its eg. because windows and linux have slightly
different ways to encode paths. 


### Obtaining the list of tables (dataframes) contained in the database

```{r}
dbListTables(dbconn)
```

The data you have registered is in the `Isolates` table.


### Obtaining data from a table of the database

Database are very convenient to store large amount of data, an amount of data
that would not fit in the memory of your computer. 

There are then two ways to work with the data: 

- sending all the data in R memory and then working on selecting and filtering the data you need to answer the research question you are interested in. However, when the amount
of data is large, it might not be possible to do so with a normal computer with limited memory. This can lead to the computer crashing, or the R session being killed. 
- sending instructions to the database to select and filter the data directly in the database, and only send the data that you need in R memory. This is what we will do here. 
This is possible due to the `dbplyr` package that allows **lazy evaluation** of 
instructions, meaning that instructions are evaluated only when needed[^1]. This 
allows to reduce memory usage by only getting the data into memory when it is needed:
- eg. when using collect() function, which pulls the data from the database into R memory. 
- eg. when requesting to View the data resulting from a query. 
Otherwise, the code is equivalent to storing the instructions (I call that the blueprint)
on how to do things without doing them[^2]. 


<!-- This will appear at the end of the webpage -->
[^1]: This is a rather complicated concept that I feel I still not master totally.
You can read [here](https://www.r-bloggers.com/2018/07/about-lazy-evaluation/).

[^2]: You can view that as `cake <- bake(cake_recipe)`. The cake_recipe is the blueprint, and the cake is the result of the baking. However the cake is only baked when you want to use it
for something eg. `collect(cake)` which is equivalent to make (`eval`) the cake now!.


- We will use the `tbl()` function from the `dbplyr` package, this will allow 
to read the data from the database... but there is a small quirk 

```{r}
isolates_tbl <- tbl(dbconn, "Isolates")
```

### Distinction lazy evaluation and evaluation

- `isolates_tbl` object contains the instructions (query) that will be sent to 
the database when we ask for the data.
```{r}
# This is the blueprint
str(isolates_tbl)
View(isolates_tbl) # Query not evaluated evaluated 
```
This gives you the source of the connection and the query that is sent to the table 


- glimpse function that we already have seen here show a different result, it actually
is evaluated and thus gives you an overview of the data obtained after sending the
query to the database.
```{r}
glimpse(isolates_tbl)
head(isolates_tbl)
```

- you can see the query that is actually sent to the database using: 
```{r}
show_query(isolates_tbl)
```
This is the translation of the query to the SQL language.
 
This means that you can now see the results of the query directly with show: 
```{r}
View(isolates_tbl %>% collect())
```

Lets look at the description of the collect function
```{r}
?collect 
```

**Force computation = force evaluation.** The query instructions are sent to the 
SQL database and the data are sent back to R. When you assign the data that
is sent back to an object, this object containing the data is then in R memory.
This becomes equivalent to working on a data frame that we read into an R object from
a spreadsheet. 

> PS: a tibble is a data frame created by the tibble R package (its a data frame format).
You do not need to bother about that here. 


```{r}
isolates_df <- collect(isolates_tbl)
str(isolates_df)
glimpse(isolates_df)
head(isolates_df)
```

### We used R memory for nothing : freeing some memory (if we have time)

Because we expect your data to be quite large, we would like to minimize memory usage,
so your computer continues to function optimally.

But for demonstration purpose, we have created a data frame `isolate_df` that is stored
in R memory. We want among other to remove this object from memory.

Note: In the environment panel, there is a little disc showing how much memory is used.


#### A bit of understanding of memory 

- Lets create two more object for demonstration purpose 

```{r}
dummy_a <- 10
dummy_b <- isolates_df
```


- listing the object in the environment
```{r}
ls()
```

- [ ]  Compare this to the objects listed in the environment panel

Did we made an identical copy of the data frame or does it point towards the
same place in the memory ? (for people who know about python, is this an hard 
copy?)

```{r}
isolates_df == dummy_b
# This is more practical 
all.equal(isolates_df, dummy_b) 
identical(isolates_df, dummy_b)

identical(isolates_df, dummy_b, ignore.bytecode = FALSE)
```

- [ ]  Read about how R compare objects [Identical function](https://rdrr.io/r/base/identical.html) and [memory and pointers](https://nonvalet.com/posts/20220316_memory_and_pointers_r/)


```{r}
dummy_c <- dummy_b
identical(dummy_b, dummy_c)
# does it modify the original object or create a copy ?
```

The objects point to the same memory, but when we reassign a modified object
then the memory location becomes different, and thus independent of the object
it was originally copied from. This is good, but then it also mean that if you
at each step creates copies of a modified object you can soon run out of memory. 

```{r}
dummy_c <- dummy_c %>% filter(PATIENT_ID == "231")
identical(dummy_b, dummy_c) # the objects are now different
```

#### Freeing memory

```{r}
ls() # listing objects

```

We want to remove only one object 
```{r}
rm(dummy_a)

ls()
```

We can see that the object has been removed from the environment. 
We want to remove several other objects:  "isolates_df, dummy_b" and dummy_c if 
you did the exercise above. 
We can do that using a vector of object
```{r}
dput(ls()) # a trick to allow creating a vector you can copy and edit

# I copy and edit the result
rm(list = c("dummy_b", "dummy_c", "isolates_df"))
ls()
```

The objects are not in the memory anymore. 

### Selecting and filtering data by sending instructions to the database. 

We can generally use the same verbs (aka functions[^3]) as in we learned in dplyr 
to filter and select data from a SQLite database using dbplyr. 

> Note that however not all dplyr verbs are available in dbplyr. 
This might be due to the fact that some dplyr functionalities cannot be directly 
translated into SQL queries. See here to see the functions that work in [dbplyr](https://dbplyr.tidyverse.org/articles/dbplyr.html)

[^3]: people call the dplyr functions verbs. Honestly I am not sure about why this
distinction is made for ... but this is a term that can be found in the documentation
and useful to know to allow get better results for our searches 


```{r}
isolates_tbl %>% 
  collect() %>%
  View()
# str()
```

#### Reminder : cleaning and checking the data 
  
```{r}
buidling_query <- 
  isolates_tbl %>% 
  # This allow to remove columns from the data that are not informative
  select(-ROW_IDX, -COUNTRY_A, -LABORATORY, -SPEC_TYPE, -SPEC_CODE, -ISOL_NUM,
         -ORG_TYPE, -COMMENT) %>%
  # Allows to rename colums by removing the prefix "X_"
  rename_with(~str_remove(., "X_")) %>%
  # I want to move the ID in the begining of the table 
  select(PATIENT_ID, INIKA_ID, SPEC_NUM, everything())


show_query(buidling_query)
```
This is still a query building 

#### Reminder : controlling data quality 
We can make a little control of the data as we have I
```{r}
# ALL ids are identical 
buidling_query %>% 
  filter(PATIENT_ID != INIKA_ID) 

# a way to separate columns
buidling_query %>% 
  select(INIKA_ID, SPEC_NUM) %>%
  collect() %>%
  separate(SPEC_NUM, into = c("ID", "SPEC_NUM"), sep = "-") %>%
  # Then we can again create a verification that the IDs are identical 
  filter(INIKA_ID != ID) 
```

**Here you can see there was probably an error of recording, the data need to be corrected
or removed, depending on the case.** (see previous session). 


#### Filtering data prior to joining tables

Is it necessary ? Well the answer is it depends.

> It is actually not necessary to filter the human data prior to join WHONET data to
KoboToolbox human data, **as long as the INIKA_ID are unique and correct**, 
the join will be correct. 
Having filtered the data prior to joining can be useful as it can make it easier
to see if what we are doing is correct and/or to detect mistakes. 


```{r}
buidling_query <- 
 buidling_query %>%
  filter(ORIGIN == "h") 
  
show_query(buidling_query)
```

Great - now I have filtered the data we wanted, the human data 
```{r}
human_lab_data <- 
  buidling_query %>%
  collect()

View(human_lab_data)
```

We have reduced the size of the data set, and it should be small enough to hold into 
the memory 

## Closing a connection

When you are finished working with the connection to the SQLite database, 
you should close it.

```{r}
DBI::dbDisconnect(dbconn)
dbconn # status is disconnected
```


# Joining tables - the different kind of joints. 

We need to combine information from two data sets. We use the test data that
you entered in WHONET and the human data for questionnaires we have used yesterday
to learn how to prepare data for analysis as an example. 


## Importing the human data that was saved in an rds file. 

We need to re-import the data we saved in the previous session. 

```{r}
human_question_data <- readRDS(here::here("results", "human_data_selection_dedup.rds")) 
```


## Different types of joints 

dplyr allow to create joints between data frames.

---

![Fig: Keys and joins](./files/join_keys.png){width=50%}


![Fig: Different types of joins](https://tavareshugo.github.io/r-intro-tidyverse-gapminder/fig/07-dplyr_joins.svg){width=50%}

</br>

---

Be careful, if you ID (or key that you use to join data is not unique, this will create 
all combination of joins). Therefore each observation (row) in each data set has to be uniquely identified, and all the columns necessary to this unique identification
need to be present in the dataset we want to join. 

Discussion: 

- [ ]  What is the difference between the different types of joins ?
- [ ]  The problem of IDs that are not unique. 


> You can read about how to [join data using dplyr here](https://rpubs.com/odenipinedo/joining-data-with-dplyr) and [here](https://tavareshugo.github.io/r-intro-tidyverse-gapminder/08-joins/index.html). 
Those links are the source of the 2 images above. 

```{r}
# will also give you the information about the other mutating joints
?left_join 
```


## Combining different data frames using mutating joints 

- Finding which columns are common to the two data frames : if they are not named
identically you need to compare those yourself
```{r}
colnames(human_question_data)
colnames(human_lab_data)
```

- inner_join allows to select only the data that are observations that 
are matching in both tables 

```{r}
my_innerjoin <- 
  human_question_data %>% 
  # selecting few columns for testing
  #  this can be used to do a short selection of the columns if not all are required
  # select(1:3) %>% 
  dplyr::inner_join(human_lab_data, by = c("INIKA_OH_TZ_ID" = "PATIENT_ID")) 

my_innerjoin %>%
  View()
```


- anti-join is very practical because it allows you to find the observations
that are not matched. It can allow you to find out if the proper data are excluded
from the join

```{r, message=TRUE}
colnames(my_innerjoin)

my_innerjoin %>%
  # the joint is done using ALL columns that are named identically in both tables
  anti_join(human_question_data) 

# Oops 
# I used the wrong order ! 
# because my_inner join will contain a subset of the general data 
            
human_question_data %>%
  # the joint is done using ALL columns that are named identically in both tables
  anti_join(my_innerjoin) 
```

This shows all the data in human_question_data that are not in my_inner join.
The message also gives you information about which columns were used to create the
join 


## Verifying that the data is consistent (if time - same principle as Yesterday)

This allows you both to detect if the common data that has been registered in
the two different tables is consistent, thus allowing to check further the 
quality of your data and to filter out unreliable data. 

```{r}
colnames(my_innerjoin)
```

Here I see that the data in `"Age__yrs" should correspond to the data in 
column. Similarly The data in the "Gender" column should correspond to the data
in "SEX". Moreover, it should be possible to retrieve the data from "AGE" from the
"DATE_BIRTH" and the date to which the data has been collected if its correct inside ... 


We will test the consistence of the data in a subset to make it easier to check 
```{r}
my_innerjoin %>%
  select(1:3, AGE, SEX) %>%
  head()

```


How hell the data is not complete. 
We do as if it was. 

> Warning: to see if the data is coherent, the types of the data must be identical
or the test of equality must account for this 

```{r}
my_innerjoin %>%
  select(1:3, AGE, SEX) %>%
  # another way to write one variable
  mutate_at(.vars = "AGE", as.integer) %>%
  mutate_at(.vars = "Gender", as.character) %>%
  # This allows to test without changing the data - but if you have many different
  # categories it is better to recode the data 
  mutate(SAME_AGE = if_else(!is.na(AGE) & Age__yrs == AGE , "inconsistent" , "missing"),
         SAME_SEX = case_when(
           Gender == "Male" & SEX == "M" ~ "ok",
           Gender == "Female" & SEX == "F"~ "ok",
           SEX == "" ~  "missing",
           TRUE ~ "inconsistent")
         ) 

```




# Reporting: tables and plotting data 

We have to few WHONET example data from human. 
We use the whole WHONET test data. The principle for doing plots remains the 
same. We will use the isolate table 

## Preparation of the data (reminder)
```{r}
glimpse(isolates_tbl)
```

- we see that all the data from WHONET is of type character, except for the 
row index (ROW_IDX) column. We will have to transform those columns into appropriate
types (as done in previous lesson) and remove the columns we will not use
to facilitate our work

NB: The easiest is to do step by step using pipes and controling that
I removed all the columns I did not need.  

```{r}
isolates_tbl %>%
  select(-ROW_IDX, - ends_with("_NAME"), -SEX, -DATE_BIRTH,- PAT_TYPE,
         -DEPARTMENT, -SEROTYPE, -MRSA, -INDUC_CLI, -ORG_TYPE, - DATE_DATA, 
         -CITY, -COUNTY, -STATE, -FOOD, -BRAND, -DATE_ADMIS, -MARKET_CAT,
         -COUNTRY_A, -LABORATORY, -ISOL_NUM, -COMMENT) %>%
  glimpse()
```

when this is ok, we can assign this to a table
```{r}
my_isolates_tbl <- 
  isolates_tbl %>%
  select(-ROW_IDX, - ends_with("_NAME"), -SEX, -DATE_BIRTH,- PAT_TYPE,
         -DEPARTMENT, -SEROTYPE, -MRSA, -INDUC_CLI, -ORG_TYPE, - DATE_DATA, 
         -CITY, -COUNTY, -STATE, -FOOD, -BRAND, -DATE_ADMIS, -MARKET_CAT,
          -COUNTRY_A, -LABORATORY, -ISOL_NUM, -COMMENT) 

# Note its still a query ! 
show_query(my_isolates_tbl) 
```
Note: I should do data quality check also (here its a test data - so I will skip it, 
BUT please do as shown in previous lesson)

```{r}
my_isolates_tbl %>%
  glimpse()
```
Now I need to transform the table to the correct types 


```{r}
# transforms empty strings (character) and "NA" to NA values 
my_isolates_tbl <- 
  my_isolates_tbl %>%

  mutate(across(everything(), 
                ~if_else(. %in% c("", "NA"), NA, .))) 

# some functions in dplyr are not implemented as sql query - I need

# to collect data to be able to continue 
my_isolates_tbl  <- 
  my_isolates_tbl %>%
  collect() 

my_isolates_tbl <- 
  my_isolates_tbl %>%
  # DATE specimen using Tanzanian time zone
  mutate(SPEC_DATE = lubridate::ymd_hms(SPEC_DATE, tz = "Africa/Addis_Ababa")) %>%
  # Need to think about transformations you want to have
  # transforms AGE in months to AGE in years 
  mutate(AGE_YEARS = as.integer(stringr::str_remove_all(AGE, "m"))/12) %>%
  select(-AGE) %>%
  # transformation of data to factor - eg. a way to select columns 
  mutate(across(matches(
    c("BETA_LACT", "ESBL", "CARBAPENEM")),
    as.factor)) %>%
  # Transforming measuring values to reals 
  mutate(across(matches(
    c("AMX_ED10", "AZM_ED15", "CRO_ED30", "CIP_ED5", "DOX_ED30", "FLR_ED30", 
      "GEN_ED10", "MEM_ED10", "OXY_ED30", "POL_ED300", "SXT_ED1_2","TYL_ED30")),
    as.numeric)) 
  

str(my_isolates_tbl) # str allows to see the levels of factors
```

<!-- A note 
PS: Getting the vector of timezone 
```{r}
OlsonNames()
# Tanzania should be this one
"Africa/Addis_Ababa"
```

--> 

Transforming character to factors depends on what you want to do. 
Factors are categorical variables. 

## grouping (reminder)

I for example want to see how many different isolates were analyzed per patient.

```{r}
my_isolates_tbl %>%
  select(PATIENT_ID, SPEC_NUM) %>%
  distinct() %>%
  group_by(PATIENT_ID) %>%
  summarize(N = n()) 
```

Making a check 

```{r}
my_isolates_tbl %>%
  select(PATIENT_ID, SPEC_NUM) %>%
  filter(PATIENT_ID == "18910")
```


## Understanding the data  (reminder )

- [ ] what is the unique way to identify each isolate ? which columns must be used ?

```{r}
test_selection <- 
  my_isolates_tbl %>%
  select(PATIENT_ID, SPEC_NUM,  
         AMX_ED10,  AZM_ED15,  CIP_ED5)

test_selection
```

I need to think how i want to split my data (I am not used to 
analyze those data, so I need to thinkg about it and plot it)
```{r}
summary(test_selection %>% select (-PATIENT_ID, -SPEC_NUM))
```


## Long format for fast plots (new)

Transforming data to long format is a nice way to rapidly make eg. boxplot
for many variables at once. 


```{r}
test_selection %>%
  # security to be sure there is no grouping
  # all isolates (no groups)
  ungroup() %>% 
  pivot_longer(cols = -c(PATIENT_ID, SPEC_NUM), 
               names_to = "Antibiotic", 
               values_to = "diameter value") %>%
  # Here is the trick to use variables with spaces in ggplot - useful at final plotting
  ggplot(aes(x = Antibiotic, y = `diameter value`)) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


## Making categories for plotting 

Example making categories of values for plotting measured values of resistances 

! Categories of resistance are artificial (eg. I assumed that there was a break 
point and that it was the same for all all antibiotics)

Note: here we transform to ordered factor, because the order is important for plotting
and has qualitative meaning. 

```{r}
test_selection2 <-  
  my_isolates_tbl %>%
  mutate(across(matches(
    c("AMX_ED10", "AZM_ED15", "CRO_ED30", "CIP_ED5", "DOX_ED30", "FLR_ED30", 
      "GEN_ED10", "MEM_ED10", "OXY_ED30", "POL_ED300", "SXT_ED1_2","TYL_ED30")),
    ~case_when(
      . < 10 ~ "Sensitive",
      . >= 10 & . < 20 ~ "Intermediate",
      . >= 20 ~ "Resistant",
      TRUE ~ "NA"
    ))) %>%
  mutate(across(matches(
    c("AMX_ED10", "AZM_ED15", "CRO_ED30", "CIP_ED5", "DOX_ED30", "FLR_ED30", 
      "GEN_ED10", "MEM_ED10", "OXY_ED30", "POL_ED300", "SXT_ED1_2","TYL_ED30")),
    factor, ordered = TRUE, levels = c("Sensitive", "Intermediate", "Resistant"))
    ) %>%
  group_by(PATIENT_ID) 
  
str(test_selection2)
str(my_isolates_tbl)
```


We can use the data in different ways.

I want to see eg. if the data is consistent for the same patient, visually.

```{r}
test_selection2 %>%
  select(PATIENT_ID, matches(
    c("AMX_ED10", "AZM_ED15", "CRO_ED30", "CIP_ED5", "DOX_ED30", "FLR_ED30", 
      "GEN_ED10", "MEM_ED10", "OXY_ED30", "POL_ED300", "SXT_ED1_2","TYL_ED30"))) %>%
  distinct() %>%
  #glimpse()
  pivot_longer(cols = -PATIENT_ID, 
               names_to = "Antibiotic", 
               values_to = "Resistance") %>%
  ggplot(aes(x = Antibiotic, y = Resistance, color = Resistance)) +
  # I want the geom_col because I want to see the values and nopt the count
  #geom_col(stat = "identity", position = position_dodge(width = 0.5)) +
  geom_point(size = 3) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~PATIENT_ID)
```

This is a way to look at your data (not the best graph though) - but it allows to
see if one ID has several values of resistance. if there are several points for 
the same antibiotic. 

It also shows your plots do not need to be perfect if its only to understand the data.


Example of heatmap

```{r}
test_selection2 %>%
  ungroup() %>%
  select(PATIENT_ID, SPEC_NUM,  matches(
    c("AMX_ED10", "AZM_ED15", "CRO_ED30", "CIP_ED5", "DOX_ED30", "FLR_ED30", 
      "GEN_ED10", "MEM_ED10", "OXY_ED30", "POL_ED300", "SXT_ED1_2","TYL_ED30"))) %>%
  distinct() %>%
  pivot_longer(cols = -c(PATIENT_ID, SPEC_NUM),
               names_to = "Antibiotic", 
               values_to = "Resistance") %>%
  arrange(PATIENT_ID) %>%
  ggplot(aes(x = Antibiotic, y =  SPEC_NUM, fill = Resistance)) +
  geom_tile()  +
  theme_bw() +
  # inverse coordinates 
  coord_flip()
```
 

# Make sure you understood correctly 

- factors 
- data types
- how to create categories (if_else and case_when are useful for that)


Look in [R for data science book](https://r4ds.hadley.nz/). 
This will help you go further. 

Other specific resources can be: 

- [ ] Factors :  [look at this lesson](https://swcarpentry.github.io/r-novice-inflammation/12-supp-factors.html)
- [ ] data types - [Look at this lesson](https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures.html) 



# Tricks

- `?tidyselect::select_helpers` to see the ways to select columns more efficiently
using helpers like `starts_with`, `ends_with`, `contains`, `matches`, `one_of`, `num_range`

- Use the course material in `Rmarkdown`, you can copy the examples of formatting
and adapt the course material to your need. 

- "pie chart" visualization is controversial in data science (its difficult to evaluate the difference between parts), however there are seen in a lot of places. Eg look [here](https://evolytics.com/blog/8-dont-use-pie-charts/#:~:text=The%20pie%20chart's%20primary%20limitation,pie%2C%20tend%20to%20become%20unreadable.) to know more !


# When you are ready to go further:

- [ggplot2 book](https://ggplot2-book.org/) all about doing graphs 

- Working with SQLite database without and with R: [Databases and SQL course](https://swcarpentry.github.io/sql-novice-survey/index.html)
- [dbplyr vignette](https://dbplyr.tidyverse.org/articles/dbplyr.html)
- [programming in the tidyverse](https://krlmlr.github.io/tidyprog/index.html)
  particularly the chapter about tidy evaluation should be usefull


## Other ressources that I either used or look at and found if could be useful for you one day
> some are questions we asked oursevles why making this course !

- [SQL query order of execution](https://www.sisense.com/blog/sql-query-order-of-operations/)
- [Lazy evaluation and lazy queries](https://smithjd.github.io/sql-pet/chapter-lazy-evaluation-queries.html)

- for programming with dplyr : read about 
  - tidy evaluation 
  - non standard evaluation
  - injection 

- [Advanced R](https://adv-r.hadley.nz/index.html)
- [bookdown](https://pkgs.rstudio.com/bookdown/) - You can easily make a book And a webiste as done in many books (easier than different Rmd files, start from R template as done for this course)

- [Tibble vs dataframe](https://posit.co/blog/tibble-1-0-0/) - "lazzy evaluation"
- [exact equality vs ==](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/identical)
<!-- 

- injection  ? vs lazy evaluation ?  
  
rlang : [qq_show](https://rlang.r-lib.org/reference/qq_show.html) 
      

- should you learn to make functions using dbplyr, [this](https://dbplyr.tidyverse.org/articles/dbplyr.html) could be of help

[tidy evaluation](https://brad-cannell.github.io/r_notes/tidy-evaluation.html)
[and](https://krlmlr.github.io/tidyprog/tidy-evaluation.html)
--> 

Back to [Index](index.html) 
