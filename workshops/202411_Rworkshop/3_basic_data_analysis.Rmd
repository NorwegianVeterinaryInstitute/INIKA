---
title: "`r params$title`"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author: Eve Zeyl Fiskebeck
params:
  title: "3. Basic R for data analysis" 
  project_path: "`r here::here()`"
output: 
  
  rmdformats::readthedown:
      
      css: style.css
      self_contained: true
      code_download: true
      toc_depth: 4
      df_print: paged
      code_folding: hide
      author: params$author
      highlight: espresso
      number_sections: true
  
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# A good organisation of your project for data analysis

```{r}
DiagrammeR::mermaid(
  "
  graph TB
  subgraph  
    A[Preparing your work]
    A --> C[Directory structure]
    C --> D[Project Setup]
    D --> D1[data]
    D --> D2[results]
    D --> D3[src OR code]
    D --> D4[bin]
    D --> D5[doc]
    D --> D6[ ... ]
  end
  subgraph  
    B[Work : one or several analyses] --> E[eg. Analysis one]
    E --> F[Code]
    F --> H[Session Setup]
    F --> I[Analysis]
    F --> K[Prettyfying]
    H --> G[Results : output]
    I --> G
    K --> G
  end
  ")
```

<u>Fig: Workflow and directory preparation for each project</u>

R project\` Directory & structure :

-   Favors good organisation
-   Allows git / version control
-   Easier to share - allows others to have the same organisation
-   Reproducibility

Example from the [Authoring scientific publications with R
Markdown](https://ucsbcarpentry.github.io/R-markdown/02-intro/index.html):

-   [ ]   `data` : **for the raw data and associated metadata (can be
    linked) ! Not under version control**
-   [ ]   `results` : processed data and results of analyses - under
    version control
-   [ ]   `src`OR `code`: for the code you write - under version control

Optional :

-   `bin` : for external code or binaries (eg. a compiled program) -
    (likely not under version control)
-   `doc` : text documents associated to the project

## Exercise: Getting organized to prepare data analysis

-   [ ]  create the data and results directories in your project
    directory
-   [ ]  copy the data from the **Human project (KoboToolbox)** into the
    data directory

# First steps: preparing our data before data analysis

-   We will use the [tidyverse](https://www.tidyverse.org/) metaverse
    (set of package what work well together) to manipulate data.

-   We will also try to show you the relation between baseR and dplyr (a
    package for data manipulation in tidyverse) to explain some basic
    programming concepts.

## Loading required packages

By **convention** they are loaded at the beginning of a script or
document. (However, it works well to load them when they are needed)

-   [ ]  load here and tidyverse packages

Unhide to see solution

```{r libraries, include=FALSE}
library(here)
library(tidyverse)
```

-   [ ]  what does meen the green ticks and red crosses ?
-   [ ]  which tidyverse packages are loaded ?
-   [ ]  can you see which version was used for each package ?

## Reading our data (our data is contained in a dataframe)


Data frames (tabular format - similar to spreadsheets) - columns :
variables (including identifiant) - raws : observations


```{r  echo=TRUE, eval=TRUE, class.source = "fold-show"}
human_data <- 
  readr::read_csv2(
    here("data", "2024-09-25_INIKA_SAMPLING_ANALYSIS_HUMAN.csv")
    )

```

We need to check: 
1. If the data has been loaded correctly 
2. If the types of the data in each columns as recognized correctly


To look at the table graphically:

-   [ ]   Environment pane \> (click) human_data (also can open in a new window)
-   [ ]   alternative with code

```{r}
View(human_data)
```

You are unlucky - you have to learn analyzing a huge table. This is not
easiest to start with. Lets proceed step by step. In the following
sections, we will transform the raw data to a dataset so it becomes be
easier to work with.

> Note: I am writting the course at the same time as I discover the
> data. Somethings might be done in different order, but I want you to
> see how I process the data, and that well, even if I am more
> experienced than you, I do not know all. So I struggle and find
> information to I can learn and go forwards.

### Exercise

-   [ ]  use the help to look at the other options to read the table. I
    used default functions for simplicity. What are those ?
-   [ ]  why did I use csv2 instead of csv? the extension of my file was
    csv right! Did I do a mistake?

------------------------------------------------------------------------

## Exploring the structure of the data

-   [ ]   what is the structure of the data ? (the data is in an object,
    so we look at the structure of the object) How many rows
    (observations) and columns (variables) ?
-   [ ]   We can look more in detail at the structure of the
    programmatically

```{r}
# we use tidyverse so better to stay in the same system
glimpse(human_data)

# but we could have used
# str(human_data) 
```

### Types of data in the columns

-   [ ]   We have different types: We have double (real) date,
    character, logical. Oops I had never seen `dttm` type before.

> It seems to be something for dates, but I will use google to find out
what it is exacly: `what is dttm R type`. I see [R for data science
page](https://r4ds.had.co.nz/dates-and-times.html) - it seems to be a
reliable source of information. I open the webpage then I search
`ddtm` in this web page and I see that it is a date-time type used by
tibbles. I know tibles are a name used to call a special type of
dataframes (tibble package in tidyverse). We do not need to bother
with further information for now.

-   [ ]   what is the type and class of human_data object? 

```{r}
typeof(human_data)
class(human_data)
```

<!-- A data frame is a special type of list. 
List with the same number of element. 
In a column, the types of each element are identical
--> 

Types of the data in columns and types of objects are different things. 

I do not change the types that need to be changed, right now. This
will be easier to do later on. The important thing is that the data appears
to have been read correctly. We will make some checks to be sure.

## Data wrangling and tyding 

### Renaming columns

I see that there is some spaces. Column names with spaces are more difficult
to work with (its not impossible, but its not practical). There are also 
other special characters `? /` that will make work with those data difficult. 
I will also remove those.

I will change the column names to remove the spaces. 
I will do that step by step, which will allow us to verify what we are doing 
at each step. 

1. I create an object that contains the column names in an object
2. I create a new object with new column names (spaces replaced by `_`)
3. I replace the column names in the table by the new column names. 

At each step we will check what we have done.

```{r}
# step 1
original_columns <- colnames(human_data)
original_columns

# step 2
# replacing spaces 
new_columns <- str_replace_all(original_columns, " ", "_")
new_columns

# replacing special characters 
new_columns <- str_replace_all(new_columns, "[?/,;.*()-]", "_")
new_columns


# removing _ at the beginning and end of column names (should not start with _)
new_columns <- str_remove_all(new_columns, "(^_*)|(_*$)")
new_columns

# this can also be done, still step by step using pipe ... like that
# space is replaced by _
# a series of , or ; or . or * or ( or )

new_columns <- 
  colnames(human_data) %>%
  str_replace_all(" ", "_") %>%
  str_replace_all( "[?/,;.*()-]", "_") %>%
  str_remove_all("(^_*)|(_*$)")

new_columns
```

There is detailed information [here](https://stringr.tidyverse.org/articles/regular-expressions.html)
on how to use regular expression with 
`stringr` package functions 


Now that we are sure the new column names will be easier to use, 
we can replace the column names in the table. 
We can still readjust those ones later on if needed. 

```{r}
colnames(human_data) <- new_columns
colnames(human_data)
```



### Selecting columns to reduce the size of the table we will work with 

Now we need to find columns with redundant information
that we will not need for the analysis. This will make the data frame easier
to work with.

we can do that by column names:
```{r}
glimpse(human_data)

human_data %>% 
  select(INIKA_OH_TZ_ID, Age__yrs_, Gender) %>%
  View(.)

```

`NA` represents missing value (its kind of a neutral filling of the cell)

or by column numbers (starting from 1), it can be easier than to type
```{r}
colnames(human_data)

human_data %>% 
  select(3,4,5) %>%
  head(.)

```


There are too many columns in the data, some with redundant data (eg.
the different Gender columns, and many others). 

Here For demonstration purposes, I select a subset of the columns 
(this will be enough)

Tricks: The tab is helpful to help you write the end of column names
(basically the questions that are detailed as 0 or 1 where there is text column
do not need to be taken in.)
```{r}
human_data_selection <- 
  human_data %>%
  select(INIKA_OH_TZ_ID, Age__yrs, Gender, Enter_a_date, Region, District, 
         Specify_if_other_district, Sample, Season, Origin_of_sample, 
         Which_class_grade_are_you, 
         Who_is_your_caretaker, 
         If_others__mention, 
         What_is_your_occupation_and_or_of_your_caretaker, 
         Have_you_ever_heard_about_AMR, If_yes__how_did_you_get_this_information, 
         Have_you_or_your_children_used_any_antibiotics_at_any_time, 
         If_yes__where_did_you_get_these_drugs_from,
         If_it_was_drug_sellers_or_pharmacy__did_you_have_a_prescription_from_the_doctor_prescriber,
         GPS_coordinates_latitude, GPS_coordinates_longitude) 

human_data_selection %>% View()
```


### Some data verification and removing one emtpy column


```{r}
glimpse(human_data_selection)
```

I see that `Specify_if_other_district` is of type logical (TRUE or FALSE) `lgl`
this is unexpected, as if something had been registered in here, there should be 
text (character type)

I suspect that nothing has been registered. I will check that. 

- This is a way to look at the content of the column only

```{r}
human_data_selection$Specify_if_other_district
```

- I can see if there is something that is not NA by making a specific subset 
of the column, and then counting the number of elements in that subset

```{r}
# create a logical vector : if is NOT NA wil register TRUE, otherwise if NA will put FALSE
# A logical vector allows to select values that are TRUE
# Example 

learn_test <- c(NA, "not empty", " ")
!is.na(learn_test)
test <- learn_test[!is.na(learn_test)]
test
length(test)
```

We do the same for the column of our dataset. 
```{r}
!is.na(human_data_selection$Specify_if_other_district)

test <- 
  human_data_selection$Specify_if_other_district[!is.na(human_data_selection$Specify_if_other_district)]

length(test)
```

Ok, this column is totally empty, we can remove it from the data set. 
A negative selection allows to remove columns.
We need to replace the data frame with the new one (reassignment)

```{r}
human_data_selection <- 
  human_data_selection %>%
  select(-Specify_if_other_district)

# verification it has been removed
colnames(human_data_selection)
```


### Changing the data types of some columns

```{r}
glimpse(human_data_selection)
```

INIKA identifier needs to be read as character (we do not want to treat it as 
a number but as a text)

The age (in years) could be an integer (whole number) instead of a double.
We will change that 

Yes/No answers can be treated as either logical (TRUE/FALSE) or as factors
We can look at the different levels (the different values, here answers, taken
by a variable)

`NA` (missing values are NOT transformed into a level by default, by default
they are excluded). Type F1 for help. 

> Here we will treat them as a level of a factor, just you show you that you need
to have critical sense, otherwise you might not detect mistakes. 
We will show you how to correct this mistakes. 

**Normally we do not use the exclude option here.**
 

```{r}
# this is a way 
human_data_selection$Have_you_ever_heard_about_AMR

levels(factor(human_data_selection$Have_you_ever_heard_about_AMR, 
                  exclude = NULL))
```

Another way is to look at the distinct values of the column
```{r}
# or this is another way using tidyverse
human_data_selection %>%
  select(Have_you_ever_heard_about_AMR) %>%
  distinct()
```

I want to treat those as factor. I will have to create a transformation
of the tables types that account for NA values. I want to know how many 
and where data are missing. 

First I will show you how you can change types, and check the result at the same time.
After we put everything together and do it in one go, and modify the table.

mutate: transform the data (it can be types, or some calculation...)

```{r}
# A way to do this
human_data_selection %>%
  mutate(INIKA_OH_TZ_ID = as.character(INIKA_OH_TZ_ID)) %>%
  glimpse()

# another way to do that
human_data_selection %>%
  mutate_at(vars(INIKA_OH_TZ_ID), as.character) %>%
  glimpse()
```

When there is only one column, its possible to write the name of the column like
that and it will work. 
```{r}
human_data_selection %>%
  mutate_at("INIKA_OH_TZ_ID", as.character)  %>%
  glimpse()
```

As you can see, there are several solutions, pick one that you understand

---

Now I want to transform many columns to factor. Because I want to create a 
summary of the table, which will be easy to look at and count the number of
observations in each category. 

I can select also by column number, this allows to select slices of columns

```{r}
colnames(human_data_selection)

human_data_selection %>%
  mutate_at(vars(3, 5:18), factor,  exclude = NULL)  %>%
  glimpse()
  
```


---

And I want to transform the age to integer (that will be useful for display
in categories)

```{r}
human_data_selection %>%
  mutate("Age__yrs" = as.integer(Age__yrs)) %>%
  glimpse()
```

---

Now we put together all the steps and modify the table. 
```{r}
human_data_selection <- 
  human_data_selection %>%
  mutate_at(vars(INIKA_OH_TZ_ID), as.character) %>%
  mutate("Age__yrs" = as.integer(Age__yrs)) %>%
  mutate_at(vars(3, 5:18), factor,  exclude = NULL) 

glimpse(human_data_selection)
```

### Summary of the data : a fast overview of the data contained in the dataset

Now we can have an overview of the data content.
We will count the number of values taken by factor and have some summary statistics
for the numerical values. 

```{r}
summary(human_data_selection)
```

Its useful but not very nice. It can be done with dplyr ... but is not as complete.
I had written a function that we can use to have a formatted summary. 
We can use external function by sourcing the file.

#### Bonus - formatting the summary table and export in a file (we do not do during the course)

This is a little quirk to make the results more readable to make it visible, 
were we transform the summary into a fake table (emtpy lines are added)

Try to load those packages, if it does not work, you need to install them, because
the function relies on those packages

```{r}
# this is the path of the file 
here::here("src", "format_summary_statistics_fun.R")
# this allows to load the function contained into the file in the memory
source(here::here("src", "format_summary_statistics_fun.R"))
```

We see in the environment panel that the `format_summary_statistics` function is now available.

We can look at the code of the function like that
```{r}
format_summary_statistics_fun
```

You can also open the file, to look at it. This is a bit to advanced for now. 
There is some explanation included in the file on how to use it. 

```{r}
format_summary_statistics_fun(human_data_selection) %>% View()
```

It is a bit more readable, if you had a `results` directory then it should be
saved there as a tsv (tabulation separated values) file.

### Filtering out unwanted values. 

Oops, we see that we have registered rows that should not be there. 
We will filter out those data from the data set. 

Another way to confirm is to look at the range of values
```{r}
human_data_selection %>%
  select(Age__yrs) %>%
  range(na.rm = TRUE)
```

how many of those data we need to remove ? 
- by transforming the data type to factor, I can have a contingency table of the 
different values of the variable (here - age). 
```{r}
factor((human_data_selection$Age__yrs), exclude = NULL) %>%
  table()
```

We see that `r 15 + 72` rows should be removed from the data set. 
<!-- inline computation can be used, when you rerun the data, this will be 
automatically updated in your text --> 

We remove the data that we do not want by applying a filter, to only select
rows from which values in a certain column contains values we want. 
```{r}
test <- 
  human_data_selection %>%
  filter(Age__yrs >= 12) 
  
nrow(human_data_selection)
nrow(test)
```

We filtered out `r nrow(human_data_selection) - nrow(test)` rows.

Oops ... We filtered one too much. We removed the row where the value was missing.
The choice of removing this value definitively depends on the context. 
Can the value be recovered from (eg. field notes ?) and then corrected HERE !. 
Will it influence our analyses or not ? 
Can it be estimated from other values ?

<!-- Madelaine needs to comment --> 

For now we will keep the NA value ( we are learning)
```{r}
test <- 
  human_data_selection %>%
  filter(Age__yrs >= 12 | is.na(Age__yrs)) 

nrow(human_data_selection) - nrow(test)  

table(factor((test$Age__yrs), exclude = NULL))
```

This now correct. We can apply this filter to the data frame. 

```{r}
human_data_selection <- 
  human_data_selection %>%
  filter(Age__yrs >= 12 | is.na(Age__yrs)) 
```

We can look again at the data content: 

```{r}
summary(human_data_selection)
```


> Note : you always need to check that you are doing the right thing.
You can also write small dataset tests to be sure that you do the correct thing.
When you have written code, it easy to rerun it when we have found our mistakes 
and corrected them. It is not possible to detect mistakes and correct them when
we correct values on a spreadsheet directly. This is why the raw data should NOT
be modified directly. 


### Selecting rows with missing data to see if we can recover them from field notes

```{r}
human_data_selection %>%
  filter(is.na(Age__yrs)) %>%
  # this allows to ensure that we see all the columns in the console 
  print(width=Inf)
```

hum, all the data of this ID appears to be missing.
We can remove this raw from the dataset 

#### Exercise : remove the row from the dataset 

Remove the row containing missing value in Age from the data set.

Hint: this means that you need to keep everything that is Not missing (put ! in 
front of is.na command).

Do not forget to do a test, and control that is does what you want. 


```{r}
# this should work  to only keep those with NA
test <- 
  human_data_selection %>%
  filter(!is.na(Age__yrs))

# control  - I check if I find those with NA again 
test %>%
  filter(is.na(Age__yrs)) %>%
  # this allows to ensure that we see all the columns in the console 
  print(width=Inf)

# OR a trick 
is.na(test$Age__yrs)
sum(is.na(test$Age__yrs))
# TRUE has a value of 1, FALSE has a value of 0
# example 
sum(c(TRUE, FALSE, TRUE))
```

NB:  `c()` function is a way to write vectors (one dimentional arrays). 

## Selecting rows that contains missing data in any column

If we have few missing data, we can go faster in our check by selecting rows
that contains missing data in any column. 


```{r}
View(human_data_selection) 
# I see some other columns with NA (show how to do) - eg who is your caretaker


human_data_selection %>%
  filter(if_any(everything(), is.na))

```

Euhh ... this does not work as intended ... why ? 

This is because I have transformed the data type of the columns to factor, 
and NA values are considered as a level. 

When we look at factors, we see characters labels. Actually factors are encoded
in R as integers. Lets try to understand this. 

```{r}
# I create a vector character with some values
test_vector <- c("1", "3", "C", NA)
test_factor <- factor(test_vector, exclude = NULL)

test_factor
as.integer(test_factor) # its a factor - it can be coerced to a number
attributes(test_factor)

as.integer(test_vector) # its character - "C" letter cannot be coerced ("1" can be)
attributes(test_vector)

# another example 
test <- factor(2:4)
test

as.integer(test) # the levels starts at 1 ... the levels are filed in order they are populated

# test 
which(levels(test_factor) == "C")
which(levels(test_factor) == NA) # this is not the correct way to test, NA is particular

is.na(levels(test_factor))
which(is.na(levels(test_factor)))
```

Ok, so we need to change the data type of the columns again, to be able to 
see which factors contains NA. 

> Note: You see why its important to have critical sense. A single error at a
line can induce error to your dataset. Therefore you need to check that 
the code you write does what you want.

To change the data types of factors, we need to revert first to characters, then
we can re-change to factor and this time NA will not be included in the levels. 

```{r}
# changing the data types (a remoinder)
test <- 
  human_data_selection %>%
  mutate_if(is.factor, as.character) %>%
  mutate_if(is.character, as.factor) 

glimpse(test)

levels(test$Who_is_your_caretaker)  # NA are not in levels 
unique(test$Who_is_your_caretaker)  # but the are listed in unique as missing values

# good this worked - so we can make the changes to the data frame
human_data_selection <- test
```

Now we can test again if we have missing values in any column. 

```{r}
human_data_selection %>%
  filter(if_any(everything(), is.na)) 
```

```{r}
human_data_selection %>%
  filter(if_any(everything(), is.na)) %>%
  dim()
```

We have missing many values in the data set, in 402 rows over 20 columns.
Lets have another look at the summary, is this distributed over all columns
or are there some columns where its expected to have many NAs ? 
```{r}
summary(human_data_selection %>% select(-INIKA_OH_TZ_ID))
```

<!-- 
[1] "Which_class_grade_are_you"                                                                  "Who_is_your_caretaker"                                                                     
[3] "If_others__mention"                                                                         "If_yes__how_did_you_get_this_information"                                                  
[5] "If_yes__where_did_you_get_these_drugs_from"                                                 "If_it_was_drug_sellers_or_pharmacy__did_you_have_a_prescription_from_the_doctor_prescriber"
--> 


"If_others__mention", "If_yes__how_did_you_get_this_information", 
"If_yes__where_did_you_get_these_drugs_from",  
"If_it_was_drug_sellers_or_pharmacy__did_you_have_a_prescription_from_the_doctor_prescriber"

are columns we can expect many NAs due to the question was asked, 
because it was a continuation of a particular case. 


### Exercise : Counting the number of NAs in each column  (we do not do)
... and using it to find column with many NAs

We can have a different approach to find which columns containing many NAs,
eg by counting the number of NAs in each column. 

```{r}
# filter columns that contain more than 10 NAs
human_data_selection %>%
  mutate(across(everything(), is.na)) %>% # this will transform the data frame to TRUE/FALSE (TRUE it its na)
  summarise_all(sum) %>% # because TRUE is 1, FALSE is 0, we can sum to count the number of NAs
  select(where(~sum(.) > 10)) %>% # now we select the columns where the sum is above 10
  colnames() # and we get the column names 

# trick comment %>% and exectue before to see what it does
```

now we have obtained the list of columns with NA that I had put in the comments
(hidden in html) above. 








# !!!! HERE 



-   str glimpse





We already have seen some data types - character (vector) - list - data
frames are a special type of list

### Distinct values 
- We need to ensure that ID is unique. If not, there is proobably 
a problem with the data.

### Object Visualisation

-   Visualization of objects in the environment and without the
    environment (eg, view, print) example of data frame

View head tail Rstudio

printinf ? find out where

note we see also other objects example functions - code of a function

allow to find \### Getting a fast summary of your data

-   inlc. maybe filter \## Data frame manipulation with dplyr

See also: [Data frame manipulation with
dplyr](https://swcarpentry.github.io/r-novice-gapminder/12-dplyr.html)

-   filtering
-   mutating
-   changing values (eg. NA and spaces)
-   selecting columns
-   group by
-   summary

## Basic plotting

# To put somewhere

-   decimals ... warning ! not done -\> do that during transformation

```{=html}
<!-- 
<mark><b> </b></mark>
-->
```
# Some usefull tricks and notes

- `Escape` when you had done typing error and that the console starts with a + (to abort your typing)
- `Ctrl + L` to clear the console
- `Ctrl + Enter` to run the current line
- `Tab` is your fried to avoid a lot of writing, it helps you auto-complete and go faster and without spelling errors 

- Do not forget about the possibility to visualize the data frame in another window, so you can look at it and write code at the same time.


---

A package that I find practical to use (but you can also search on
the internet instead) when I do not know what functions a package
contains is the package pacman. It allows you to look at which functions
are contained in a package.

```{r}
library(pacman)
p_functions(dplyr)
```

You can also use the embedded help function in Rstudio for that:

```text
?dplyr # gives you info on where to find info
dplyr:: # and use the arrows to explore the different functions contained in the package
```

---

There are several exercises you can do here on your own. 
Try to understand what it does and use the help. 
I let those exercises because you should be able to modify them to use with other variables
which can help you start (and avoid searching a lot on the internet). 

> I had actually to google to find out how to do things. And even if I have some experience
with using R, I still control that the code is doing what it is supposed to do. Error
is human ! 


# When you are ready to go further:

You can use those lessons to repeat what you have learned (but in
another way) and go further in your learning

-   [R for Reproducible Scientific
    Analysis](https://swcarpentry.github.io/r-novice-gapminder/)

-   [Programming with
    R](https://swcarpentry.github.io/r-novice-inflammation/)

-   [Programming with Databases -
    R](https://swcarpentry.github.io/sql-novice-survey/11-prog-R.html)

The [CRAN](https://cran.r-project.org/) website you can find the reference manuals
for R programming language. If you really want to understand the basics of the language
you need to start reading the "introduction to R" manual. 
